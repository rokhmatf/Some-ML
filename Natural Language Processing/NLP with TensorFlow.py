# -*- coding: utf-8 -*-
"""Dicoding Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MaT2y-YX8bxLiQFS78OHAStVaGJrcIVr
"""

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

import nltk
import os
import re
import string

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

df = pd.read_csv('/content/drive/MyDrive/Dataset/bbc-news-data.csv', sep='\t')
df = df.drop(columns=['filename'])
df

nltk.download('wordnet')
nltk.download('stopwords')

## lematization
lemmatizer = WordNetLemmatizer()

def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    df.title = df.title.apply(lambda x: lem(x))
    df.content = df.content.apply(lambda x: lem(x))

# removing stopword
st_words = stopwords.words()
def stopword(data):
    return(' '.join([w for w in data.split() if w not in st_words ]))
    df.title = df.title.apply(lambda x: stopword(x))
    df.content = df.content.apply(lambda x: lem(x))

# removing functuation
def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    df_new.title = df_new.title.apply(lambda x: cleaner(x))
    df_new.content = df_new.content.apply(lambda x: lem(x))

# One-hot encoding

category = pd.get_dummies(df.category)
df_encoding = pd.concat([df, category], axis=1)
df_encoding = df_encoding.drop(columns='category')
df_encoding

# Change dataframe values

bbcNews = df_encoding['title'].values + '' + df_encoding['content'].values
label = df_encoding[['business', 'entertainment', 'politics', 'sport', 'tech']].values

# Split data

bbcNews_train, bbcNews_test, label_train, label_test = train_test_split(bbcNews, label, test_size=0.2, shuffle=True)

# Apply tokenizer

tokenizer = Tokenizer(num_words=5000, oov_token='x', filters=';<=>@%&()|}*+!"#$^_,-:[\]`{./~ ')
tokenizer.fit_on_texts(bbcNews_train) 
tokenizer.fit_on_texts(bbcNews_test)
 
sequence_train = tokenizer.texts_to_sequences(bbcNews_train)
sequence_test = tokenizer.texts_to_sequences(bbcNews_test)
 
pad_train = pad_sequences(sequence_train) 
pad_test = pad_sequences(sequence_test)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(optimizer='adam',
              metrics=['accuracy'],
              loss='categorical_crossentropy',)

model.summary()

# callback

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.93 and logs.get('val_accuracy')>0.93):
      self.model.stop_training = True
      print("\nThe accuracy has reached more than 93%!")
callbacks = myCallback()

number_epochs = 40
history = model.fit(pad_train, label_train, epochs=number_epochs, 
                    validation_data=(pad_test, label_test), verbose=2,
                    callbacks=[callbacks])

score = model.evaluate(pad_train, label_train)

print('Loss: {:.4f}'.format(score[0]))
print('Accuracy: {:.4f}'.format(score[1]))

score = model.evaluate(pad_test, label_test)

print('Loss: {:.4f}'.format(score[0]))
print('Accuracy: {:.4f}'.format(score[1]))

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(number_epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='upper left')
plt.title('Training and Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.ylabel('loss')
plt.xlabel('epoch')

plt.show()

"""Rokhmat Febrianto

Bergabung sejak 16 Mar 2021

Kabupaten Sidoarjo, Jawa Timur 
"""